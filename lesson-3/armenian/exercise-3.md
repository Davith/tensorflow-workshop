## Առաջադրանք 3: Սովորում ենք կշիռները

Արդեն պատրաստել ենք տվյալները ու կառուցել ենք նեյրոնային ցանցը։ Շատ բան չի մնացել։ Ոնց որ արել նախքին դասերում, ընդամենը պիտի սահմանենք նպատակային (loss) ֆունկցիան ու օպիտիզատոր օգտագործենք որ կորստի արժեքը հասցնենք նվազագույնի։

### Քայլ 1:
Որպես նպատակային ֆունկցիան, օգտագործելու ենք գուշակած ու իրական գնահատականների տարբերությունների քարակուսիների միջինը։ Ավելի հստակ, օգտագործելու ենք հետևյալ բանաձևը։

**loss = (1/num_examples) * Σ(actual - predicted)<sup>2</sup>**

**actual**-ը նորմալիզացված output-ն է որը ստեղծել են **Առաջադրանք 1**-ում. **predicted**-ը հավասար է **Առաջադրանք 2**-ի **activation_2**-ին։ **num_examples**-ը օրինակների քանակն է (այս դեպքում ունենք 10 օրինակ)։

Որպեսզի ստանանք բոլոր օրինակների գումարը (Σ), կարող ենք օգտագործել tensorflow-ի tf.reduce_sum ֆունկցիան ([documentation](https://www.tensorflow.org/api_docs/python/tf/reduce_sum)).

### Քայլ 2:
Մեզ պետք է օպտիմիզատոր որը փոփոխականները (**կշիռները** ու **շեղումները**) փոփոխելով կորուստը կհասցնի նվազագույնի։ Ստեղծի GradientDescentOptimizer օբյեկտ (կոնստրուկտորին պիտի տանք learning_rate արգումենտ)։ Օպտիմիզատոր օբյեկտը ունի minimize() մեթոդ, որը որպես արգումենտ ընդունում է նպատակային ֆունկցիա ու վերադարցնում է գործողություն։ Այդ գործողությունը կատարելով մի քայլ կմոտենանք մեր ուզեցած արդյունքին։

### Քայլ 3:
**with**-ով սեսիա ստեղծի։ Այդ կոնտեքստի մեջ ենք կատարելու հետևյալ քայլերը։

```python
with tf.Session() as sess:
```

### Քայլ 4:
tf.global_variables_initializer() գործողությունը կատարի որպեսզի սկզբնական արժեքները վերագրվեն բոլոր Variable-ներին։

### Քայլ 5:
Ցիկլով 10000 անգամ կատարի **Քայլ 2**-ում ստեղծած ուսուցման գործողությունը. Հիշի որ պիտի **Առաջադրանք 1**-ում ստեղծած dictionary-ն օգտագործենք ցանցը տվյլաներով սնելու համար։

```python
  sess.run(train_step, feed_dict = feed_dict)
```

**Կեցցե՛ս։ Առաջին անգամ կառուցել ու ուսուցանել էս նեյրոնային ցանց։**
